\documentclass{report}

\usepackage{amsmath}
\usepackage{amsfonts}

\title{Regression Analysis}
\author{Arvinder Singh}
\date{March 2023}

\begin{document}
\maketitle
\tableofcontents

\chapter{Linear Regression}
%____________________________________________________%
\section{Problem Setup}
Assume a dataset \(\{\mathbf{X}, \mathbf{Y}\}\) where
\[
	\mathbf{X} = \begin{bmatrix}
    x^{(1)}_1 & \dots & x^{(1)}_d \\
    \vdots & \ddots & \vdots \\
    x^{(n)}_1 & \dots & x^{(n)}_d
	\end{bmatrix}
	\text{,}\qquad
	\mathbf{Y} = \begin{bmatrix}
    y^{(1)}\\
    \vdots\\
    y^{(n)}
	\end{bmatrix}
\]
the discrete or continuous independent variable \(\mathbf{X} \in \mathbb{R}^{n\times d}\) 
and the continuous dependent variable \(\mathbf{Y} \in \mathbb{R}^{n\times 1}\),
\(n\) is the number of training examples and \(d\) is the number of features.

The goal is to use a supervised learning approach for a regression application by learning a model hypothesis \(h_\beta(x)\) which predicts \(\hat y^{(i)}\) given \(\mathbf{x}^{(i)}\), assuming the relationship between the two is linear.

%____________________________________________________%
\section{Model}
For simple (a.k.a. univariate) linear regression, we can model the distribution of the \(i^\text{th}\) example using the following equation:
\[
	\boxed{h_\beta(x^{(i)}) = \beta_0 + \beta_1 x_1^{(i)} + \dots +  x_d^{(i)} = \beta_0 + \sum_{j=1}^d\beta_j  x_j^{(i)}}
\]
\marginpar{The term \(\beta_0\) makes the function affine.}where \(i = (1,\dots,n)\), \(j = (1,\dots,d)\), \(\beta_0\)(a.k.a. bias or intercept) represents the average of \(\mathbf{Y}\) when all \(\mathbf{X} = 0\), \(\beta_j\) (a.k.a. coefficient or slope) represents the average effect of one-unit increase of a certain feature \(j\) on \(\mathbf{Y}\)

%____________________________________________________%
\section{Loss Function}
In order to qunaitfy how well our model performs, we must define a function that calculates how "far" our model predictions \(\hat y\) were from the actual label \(y\). We would call this our "loss" function. One way define our loss is to calculate the distance between the two values. We will call this distance the residual \(r^{(i)}\), where \[
	r^{(i)} = y^{(i)} - (\beta_0 + \beta_1 x^{(i)})
\]
We would like the residuals to be close to zero.

%____________________________________________________%
\section{Cost Function}
We would like to find the loss for each example to give us an average "fit" of the model. We can do so by summing the absolute values of our residuals and dividing the sum by the number of examples. The caveat is, absolute values are a non-differentiable functions, which is not desirable. We can instead choose to average the sum of squared residuals instead, keeping the function differentiable, with an added benefit of penalizing larger losses over smaller ones.

Note: Dividing the sum of squared residuals by \(n\) gives us a biased estimate of the variance of the unobserved errors. To remove the bias, divide the sum of squared residuals by \(\mathrm{df} = n - p - 1\) where \(\mathrm{df}\) is the degrees of freedom, \(p\) is the number of parameters being estimated (excluding the intercept)

To find the optimal values for \(\beta\), we must minimize the sum of the squared residuals. That sum can be divided by \(n\) and give us the mean of the squared residuals (a.k.a. Mean Squared Error).
\[
	\boxed{\mathcal{E} = \frac{1}{n}\sum_{i=1}^nr_i^2}
\]

%____________________________________________________%
\section{Training}
Now that we have a model, along with a function to evaluate the fit, we would like to find the optimum values of our parameters \(\beta\) that would give us the least average loss. Fortunately, for linear regression problems, there is a direct, closed-form solution that we can employ. We will also take a look at optimizing our parameters using gradient descent.

\subsection{Direct Solution}
In the case of a simple (univariate) linear regression, we know that the minimum cost occurs when the partial derivative of the cost function with respect to the parameters \(\beta_0\) and \(\beta_1\) are at 0. We can use this information to estimate our parameters \(\hat\beta_0\) and \(\hat\beta_1\).

Note: Recall from single variable calculus that (assuming a function is differentiable) the minimum \(x^*\) of a function \(f\) has the property that the derivative \(\frac{\mathrm{d}f}{\mathrm{d}x}\) is zero at \(x = x^*\). Note that the converse is not true: if \(\frac{\mathrm{d}f}{\mathrm{d}x} = 0\), then \(x^*\) might be a maximum or an inflection point, rather than a minimum. But the minimum can only occur at points that have derivative zero.

\begin{align*}
	\intertext{Partial derivative of $\hat\beta_0$:}
	\frac{\delta\mathcal{E}}{\delta{\hat\beta_0}} &= \frac{\delta}{\delta{\hat\beta_0}}\frac{1}{n}\sum_{i=1}^n \left(y^{(i)} - \hat y^{(i)}\right)^2 \\
												  &= \frac{\delta}{\delta{\hat\beta_0}}\frac{1}{n}\sum_{i=1}^n \left[y^{(i)} - \left(\hat\beta_0 + \hat\beta_1 x^{(i)}\right)\right]^2 \\
												  &= \frac{-2}{n}\sum_{i=1}^n \left(y^{(i)} - \hat\beta_0 - \hat\beta_1 x^{(i)}\right)\\
												  &= \frac{-2}{n}\left(\sum_{i=1}^ny^{(i)} - n\hat\beta_0-\hat\beta_1\sum_{i=1}^nx^{(i)}\right) \\
												  &= -2\left(\frac{n}{n}\hat\beta_0+\hat\beta_1\frac{1}{n}\sum_{i=1}^nx^{(i)}-\frac{1}{n}\sum_{i=1}^ny^{(i)}\right)\\
												  &= -2\left(\hat\beta_0+\hat\beta_1\bar x-\bar y\right)\\
	\intertext{Setting partial derivative to 0 and solving for $\hat\beta_0$:}
												  &	-2\left[\hat\beta_0+\hat\beta_1\bar x-\bar y\right] = 0\\
												  &\hat\beta_0 = \bar{y} - \hat\beta_1\bar{x}
\end{align*}

\begin{align*}
	\intertext{Partial derivative of $\hat\beta_1$:}
	\frac{\delta\mathcal{E}}{\delta{\hat\beta_1}} &= \frac{\delta}{\delta{\hat\beta_1}}\frac{1}{n}\sum_{i=1}^n \left(y^{(i)} - \hat y^{(i)}\right)^2 \\
	&= \frac{\delta}{\delta{\hat\beta_1}}\frac{1}{n}\sum_{i=1}^n \left(y^{(i)} - \left(\hat\beta_0 + \hat\beta_1 x^{(i)}\right)\right)^2 \\
	&= \frac{-2}{n}\sum_{i=1}^n (x^{(i)})\left(y^{(i)} - \hat\beta_0 - \hat\beta_1 x^{(i)}\right) \\
	&= \frac{-2}{n}\sum_{i=1}^n \left(x^{(i)}y^{(i)} - (\bar{y} - \hat\beta_1\bar{x}) x^{(i)}- \hat\beta_1 x^{(i)^2}\right) \\
	&= \frac{-2}{n}\left( \sum_{i=1}^n x^{(i)}y^{(i)} - \bar{y}\sum_{i=1}^nx^{(i)} + \hat\beta_1\bar{x}\sum_{i=1}^nx^{(i)}- \hat\beta_1 \sum_{i=1}^nx^{(i)^2} \right) \
	\intertext{Setting partial derivative to 0 and solving for $\hat\beta_1$:}
	&\left( -\sum_{i=1}^n x^{(i)}y^{(i)} + \bar{y}\sum_{i=1}^nx^{(i)} - \hat\beta_1\bar{x}\sum_{i=1}^nx^{(i)} +  \hat\beta_1 \sum_{i=1}^nx^{(i)^2} \right) = 0 \\
	\hat\beta_1&\left(\bar{x}\sum_{i=1}^nx^{(i)}-\sum_{i=1}^nx^{(i)^2}\right) = \bar{y} \sum_{i=1}^nx^{(i)} - \sum_{i=1}^nx^{(i)}y^{(i)} \\
	\hat\beta_1 &= \frac{\bar{y} \sum_{i=1}^nx^{(i)} - \sum_{i=1}^nx^{(i)}y^{(i)}}{\bar{x}\sum_{i=1}^nx^{(i)}-\sum_{i=1}^nx^{(i)^2}} \\
				&= \frac{n\bar{y} \bar x - \sum_{i=1}^nx^{(i)}y^{(i)}}{n\bar{x}^2-\sum_{i=1}^nx^{(i)^2}} =  \frac{\sum_{i=1}^nx^{(i)}y^{(i)} - n\bar{y} \bar x}{\sum_{i=1}^nx^{(i)^2} - n\bar{x}^2}
\intertext{To simplify:}    
\sum_{i=1}^nx^{(i)}y^{(i)} - n\bar{y} \bar x &= \sum_{i=1}^n(x^{(i)}-\bar x)(y^{(i)} - \bar{y}) := \mathrm{S_{XY}}\\
\sum_{i=1}^nx^{(i)^2} - n\bar{x}^2 &= \sum_{i=1}^n(x^{(i)} - \bar{x})^2 := \mathrm{S_{XX}}\\
\hat\beta_1 &= \frac{\mathrm{S_{XY}}}{\mathrm{S_{XX}}}
\end{align*}

\subsection{Gradient Descent}
The other way to find the optimized values of our parameters is to use an iterative pproach via gradient descent. The basic algorithm of gradient descent is as follows:

\begin{itemize}
	\item Initialize all parameters \(\beta\) to some random value
	\item Repeat until convergence:
\begin{itemize}
	\item Choose a new value for \(\beta\) to reduce \(\mathcal{E}(\beta)\) toward the direction of steepest descent.
	\item Simultaneously update for \(j = (0, \dots, d)\)
\end{itemize}
\end{itemize}
The gradient descent step is formulated by the following function:
\[
	\beta_j := \beta_j - \alpha \frac{\delta}{\delta\beta_j}\mathcal{E}(\beta)
\]
For simple linear regression, let's assume \(\beta_0\) is included in \(\beta_j\). We can assume this if we assume \(x_0 = 1\). This will keep consistency in our formulas. So, for each \(j\) 
\begin{align*}
	\frac{\delta\mathcal{E}}{\delta\beta_j} &= \frac{\delta}{\delta\beta_j}\frac{1}{2n}\sum_{i=1}^n \left(\hat y^{(i)} - y^{(i)}\right)^2 \\
										  &= \frac{\delta}{\delta\beta_j}\frac{1}{2n}\sum_{i=1}^n \left(\sum_{k=0}^d\beta_k + x_k^{(i)} - y^{(i)}\right)^2 \\
										  &= \frac{1}{n}\sum_{i=1}^n (x_j^{(i)})\left(\sum_{k=0}^d\beta_k + x_k^{(i)} - y^{(i)}\right) \\
\end{align*}

Thus, for all training examples:
\[
	\beta_j := \beta_j - \alpha \sum_{i=1}^n x_j^{(i)}\left(\sum_{k=0}^d\beta_k + x_k^{(i)} - y^{(i)}\right)
\]
Repeat the above until convergence and update for \(j = (0,1,\dots,d)\)
Note that \(\mathcal{L}(\beta)\) is a quadratic equation and \(\alpha\) is an empirical value. The above equation is also called batch gradient descent. All data is summed at every step, which is very ineffective for big datasets. Alternatively, you can use Stochastic Gradient Descent (SGD): 

\begin{align*}
	\text{Repeat \{} \\
	&\text{For }i=1 \text{ to } n \text{ \{} \\
	&\beta_j := \beta_j - \alpha\left[h_\beta x^{(i)} - y^{(i)}\right]x_j^{(i)}\\
	&\text{\}} \\
	\text{\}}
\end{align*}
The above takes derivative of one example at a time. The gradient takes a bit more noisy approach toward minima and it never converges (unlike batch gradient descent).



\subsection{Probabilistic Solution}

%____________________________________________________%
\section{Evaluation}

\chapter{Logistic Regression}



\end{document}

